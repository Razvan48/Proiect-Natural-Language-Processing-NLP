{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Prerequisites"
      ],
      "metadata": {
        "id": "MC4rvJ8W8sI_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pTmlr9Gj5Qi-"
      },
      "outputs": [],
      "source": [
        "!pip install transformers datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
        "import torch\n",
        "import re"
      ],
      "metadata": {
        "id": "077rk-JV8fk9"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Import dataset"
      ],
      "metadata": {
        "id": "99lkdkpQ8uLB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "TAzjzdDG8pFP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Positive Examples (Keep only entries where the Rating column is not null)\n",
        "normalized_jester_df = pd.read_csv('/content/drive/MyDrive/Proiect NLP/Datasets/Preprocessed-Datasets/Positive-Examples/jester/normalized_jester.csv')\n",
        "normalized_reddit_jokes_df = pd.read_csv('/content/drive/MyDrive/Proiect NLP/Datasets/Preprocessed-Datasets/Positive-Examples/joke-dataset/normalized_reddit_jokes.csv')\n",
        "normalized_stupidstuff_df = pd.read_csv('/content/drive/MyDrive/Proiect NLP/Datasets/Preprocessed-Datasets/Positive-Examples/joke-dataset/normalized_stupidstuff.csv')\n",
        "\n",
        "normalized_jester_df = normalized_jester_df[normalized_jester_df['Rating'].notna()]\n",
        "normalized_reddit_jokes_df = normalized_reddit_jokes_df[normalized_reddit_jokes_df['Rating'].notna()]\n",
        "normalized_stupidstuff_df = normalized_stupidstuff_df[normalized_stupidstuff_df['Rating'].notna()]\n",
        "\n",
        "df = pd.concat([normalized_jester_df, normalized_reddit_jokes_df, normalized_stupidstuff_df], ignore_index=True)\n",
        "df = df.dropna(subset=['Body'])\n",
        "df = df[df['Body'].str.strip() != '']\n",
        "\n",
        "\n",
        "def is_clean(text):\n",
        "    banned = [\"fuck\", \"shit\", \"sex\", \"rape\"]\n",
        "    return not any(bad in text.lower() for bad in banned)\n",
        "\n",
        "def preprocessed_sample(sample):\n",
        "    sample = str(sample)\n",
        "    sample = sample.replace('\\r', ' ').replace('\\n', ' ')  # Replace line breaks with space\n",
        "    sample = re.sub(r'[^a-zA-Z0-9.,!?\\'\\\";:()\\[\\]{}-]', ' ', sample)  # Keep common punctuation\n",
        "    sample = sample.lower()\n",
        "    sample = re.sub(r'\\s+', ' ', sample).strip()  # Normalize spaces\n",
        "    return sample\n",
        "\n",
        "df['Body'] = df['Body'].apply(preprocessed_sample)\n",
        "df = df[df['Body'].apply(is_clean)]\n",
        "df = df[df['Rating'] > 0.5]\n",
        "print(len(df))"
      ],
      "metadata": {
        "id": "pUD73ikK8_KG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc2d3dd9-1f83-4c7e-f8ba-a57177629cde"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2282\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prepare a text file of only your positive jokes"
      ],
      "metadata": {
        "id": "HLo4MNE39RYU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "# — after your df filtering —\n",
        "# df already has only positive, clean jokes in df['Body']\n",
        "\n",
        "def split_joke(text: str):\n",
        "    # Heuristic: split on the last sentence-ending punctuation\n",
        "    # (?!.*[.!?]) ensures we grab the final sentence as punchline\n",
        "    parts = re.split(r'(?<=[\\.!?])\\s+(?!.*[\\.!?])', text.strip())\n",
        "    if len(parts) == 1:\n",
        "        # If no clear split, treat first half as setup\n",
        "        mid = len(text)//2\n",
        "        return text[:mid].strip(), text[mid:].strip()\n",
        "    setup = \" \".join(parts[:-1])\n",
        "    punch = parts[-1]\n",
        "    return setup.strip(), punch.strip()\n",
        "\n",
        "# Apply split_joke and drop any that failed\n",
        "jokes = df['Body'].tolist()\n",
        "pairs = [split_joke(j) for j in jokes if len(j) > 20]\n",
        "setups, punchlines = zip(*pairs)\n",
        "\n",
        "# Write out JSONL for HuggingFace style fine-tuning\n",
        "import json\n",
        "out = []\n",
        "for s, p in zip(setups, punchlines):\n",
        "    out.append({\"prompt\": s, \"completion\": \" \" + p})\n",
        "\n",
        "Path(\"jokes_pairs.jsonl\").write_text(\n",
        "    \"\\n\".join(json.dumps(x) for x in out),\n",
        "    encoding=\"utf-8\"\n",
        ")"
      ],
      "metadata": {
        "id": "6YpkriVw9ST2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b61c9d44-eaa4-4b3d-a5de-23884073042d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1599026"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load GPT-2 and tokenizer\n"
      ],
      "metadata": {
        "id": "dV1XcVwi9gYv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "tokenizer.pad_token = tokenizer.eos_token  # needed for batching\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")"
      ],
      "metadata": {
        "id": "dlgf6Nr09Tfp",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create Dataset and DataCollator"
      ],
      "metadata": {
        "id": "kRsZyhjA9yeS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset, DatasetDict\n",
        "from transformers import DataCollatorForLanguageModeling, TrainingArguments, Trainer\n",
        "\n",
        "# 1. Load JSONL\n",
        "ds = load_dataset(\"json\", data_files=\"jokes_pairs.jsonl\", split=\"train\")\n",
        "\n",
        "# 2. Tokenize\n",
        "def tokenize_batch(example):\n",
        "    # we concatenate prompt + completion so the model learns the whole sequence\n",
        "    full = example[\"prompt\"] + example[\"completion\"]  # note the space in completion\n",
        "    tokens = tokenizer(full, truncation=True,\n",
        "                       max_length=128, padding=\"max_length\")\n",
        "    # labels = copy of input_ids, but we mask the prompt region with -100\n",
        "    labels = tokens[\"input_ids\"].copy()\n",
        "    # Calculate the length of the prompt within the tokenized sequence\n",
        "    prompt_len = len(tokenizer(example[\"prompt\"], add_special_tokens=False, truncation=True, max_length=128)[\"input_ids\"])\n",
        "    # mask prompt tokens so loss only on punchline\n",
        "    # Ensure prompt_len does not exceed the length of labels\n",
        "    prompt_len = min(prompt_len, len(labels))\n",
        "    for i in range(prompt_len):\n",
        "        labels[i] = -100\n",
        "    tokens[\"labels\"] = labels\n",
        "    return tokens\n",
        "\n",
        "tokenized = ds.map(tokenize_batch,\n",
        "                   remove_columns=ds.column_names,\n",
        "                   batched=False)\n",
        "\n",
        "# 3. Data collator\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer, mlm=False\n",
        ")"
      ],
      "metadata": {
        "id": "srVPNekf9n2O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set up Trainer"
      ],
      "metadata": {
        "id": "dgLqSP3S-Dp6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(\"CUDA is available. Training on GPU.\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"CUDA is not available. Training on CPU.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cx7wlcM_ZXJu",
        "outputId": "1684a3ae-2ee8-4118-a689-ce6fbb2dfd39"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA is available. Training on GPU.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./gpt2-jokes\",\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=4,\n",
        "    save_steps=500,\n",
        "    save_total_limit=2,\n",
        "    logging_steps=100,\n",
        "    learning_rate=5e-5,\n",
        "    # Specify the device for training\n",
        "    fp16=True, # Enable mixed precision training (if supported by your GPU\n",
        "    **{\"no_cuda\": False} # for cuda\n",
        ")\n",
        "\n",
        "# Move the model to the selected device\n",
        "model.to(device)\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "# 5. Fine-tune!\n",
        "trainer.train()\n",
        "trainer.save_model(\"./gpt2-jokes\")\n",
        "tokenizer.save_pretrained(\"./gpt2-jokes\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 973
        },
        "collapsed": true,
        "id": "u4XPQZId95lA",
        "outputId": "44716789-93c6-4fba-92a6-06a181c0a23e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize?ref=models\n",
            "wandb: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmihneavicentiu\u001b[0m (\u001b[33mmihneavicentiu-bucharest-university-of-economic-studies\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250512_094849-7xl6x4uk</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mihneavicentiu-bucharest-university-of-economic-studies/huggingface/runs/7xl6x4uk' target=\"_blank\">./gpt2-jokes</a></strong> to <a href='https://wandb.ai/mihneavicentiu-bucharest-university-of-economic-studies/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/mihneavicentiu-bucharest-university-of-economic-studies/huggingface' target=\"_blank\">https://wandb.ai/mihneavicentiu-bucharest-university-of-economic-studies/huggingface</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/mihneavicentiu-bucharest-university-of-economic-studies/huggingface/runs/7xl6x4uk' target=\"_blank\">https://wandb.ai/mihneavicentiu-bucharest-university-of-economic-studies/huggingface/runs/7xl6x4uk</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1701' max='1701' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1701/1701 04:00, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>3.499200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>3.319300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>3.253400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>3.227400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>3.260100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>3.143800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>2.894700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>2.887200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>2.920700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>2.864900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1100</td>\n",
              "      <td>2.921000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>2.798600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1300</td>\n",
              "      <td>2.724500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1400</td>\n",
              "      <td>2.699900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>2.743900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1600</td>\n",
              "      <td>2.713600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1700</td>\n",
              "      <td>2.727400</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('./gpt2-jokes/tokenizer_config.json',\n",
              " './gpt2-jokes/special_tokens_map.json',\n",
              " './gpt2-jokes/vocab.json',\n",
              " './gpt2-jokes/merges.txt',\n",
              " './gpt2-jokes/added_tokens.json')"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Save to a local folder (inside Colab VM)\n",
        "model.save_pretrained(\"./gpt2-jokes\")       # saves pytorch_model.bin and config.json\n",
        "tokenizer.save_pretrained(\"./gpt2-jokes\")   # saves vocab and special-tokens files\n",
        "\n",
        "# 2. (Optional) Copy that folder into your Google Drive for persistence\n",
        "!cp -r ./gpt2-jokes \"/content/drive/MyDrive/Proiect NLP/models/gpt2-jokes\""
      ],
      "metadata": {
        "id": "rjEXsXYUOrUB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Generate jokes\n",
        "model.eval()\n",
        "prompt = \"Yo mamma\"\n",
        "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
        "\n",
        "# Generate 5 joke variants\n",
        "outputs = model.generate(\n",
        "    input_ids,\n",
        "    max_length=30,\n",
        "    do_sample=True,\n",
        "    temperature=0.94,\n",
        "    top_p=0.9,\n",
        "    num_return_sequences=5,\n",
        "    pad_token_id=tokenizer.eos_token_id\n",
        ")\n",
        "\n",
        "def cut_at_last_punctuation(text):\n",
        "    match = re.search(r'(.+[.!?])', text)\n",
        "    return match.group(1).strip() if match else text.strip()\n",
        "\n",
        "for i, out in enumerate(outputs, 1):\n",
        "    text = tokenizer.decode(out, skip_special_tokens=True)\n",
        "    clean = cut_at_last_punctuation(text)\n",
        "    print(f\"{i}. {clean}\")"
      ],
      "metadata": {
        "id": "3V7mZA7lXkID",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aaefada2-7033-4240-aa68-27c7599cdf9e"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. Yo mamma so fat she stuck her head out of the window on a mule!\" said the young lady. \"what?\n",
            "2. Yo mamma so fat she lost her weight! her husband is like, \"damn! she lost it!\n",
            "3. Yo mamma so fat she walked into a limo, put on a pair of white panties, and got in the way of a car.\n",
            "4. Yo mamma so dumb she walked over to the kitchen and said, \"i can't do that\" and got out her cell phone.\n",
            "5. Yo mamma so stupid, that she threw a birthday party for her 10th birthday party, and all the guests were invited to join her.\n"
          ]
        }
      ]
    }
  ]
}