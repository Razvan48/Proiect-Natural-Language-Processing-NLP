{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "2CcWIUO8VDLg"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "import transformers\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wBJJbLrGXBZx",
        "outputId": "8b1dd499-8d4a-4098-af11-00156cbade0e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "csv_file = pd.read_csv('/content/drive/MyDrive/Proiect NLP/Preprocessed-Datasets/shortjokes/shortjokes.csv')\n",
        "\n",
        "data = csv_file['Body'].head(2000).to_numpy()\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "Usye6XIAl-0o"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gpt2_tokenizer = transformers.GPT2Tokenizer.from_pretrained('gpt2')\n",
        "gpt2_tokenizer.pad_token = gpt2_tokenizer.eos_token\n",
        "\n",
        "gpt2_model = transformers.GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "gpt2_model.resize_token_embeddings(len(gpt2_tokenizer))\n",
        "gpt2_model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C6AcfK39aGxN",
        "outputId": "b2e5a5d8-a523-4ec7-d9c6-a9ebf4cee993"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT2LMHeadModel(\n",
              "  (transformer): GPT2Model(\n",
              "    (wte): Embedding(50257, 768)\n",
              "    (wpe): Embedding(1024, 768)\n",
              "    (drop): Dropout(p=0.1, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0-11): 12 x GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D(nf=2304, nx=768)\n",
              "          (c_proj): Conv1D(nf=768, nx=768)\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D(nf=3072, nx=768)\n",
              "          (c_proj): Conv1D(nf=768, nx=3072)\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class JokesDataset(Dataset):\n",
        "  def __init__(self, jokes, tokenizer, max_length):\n",
        "    self.jokes = jokes\n",
        "    self.tokenizer = tokenizer\n",
        "    self.max_length = max_length\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.jokes)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    joke = self.jokes[idx]\n",
        "\n",
        "    joke_encoding = self.tokenizer(\n",
        "        joke,\n",
        "        truncation=True,\n",
        "        padding='max_length',\n",
        "        max_length=self.max_length,\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "\n",
        "    input_ids = joke_encoding['input_ids'].squeeze(0)\n",
        "    attention_mask = joke_encoding['attention_mask'].squeeze(0)\n",
        "\n",
        "    return {\n",
        "        'input_ids': input_ids,\n",
        "        'attention_mask': attention_mask\n",
        "    }"
      ],
      "metadata": {
        "id": "75SKmKD-a4Pt"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = JokesDataset(data, gpt2_tokenizer, 512)\n",
        "\n",
        "dataloader = DataLoader(dataset, batch_size=8, shuffle=True)"
      ],
      "metadata": {
        "id": "IDO0tzt-k50n"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, dataloader, learning_rate, num_epochs):\n",
        "\n",
        "  model.train()\n",
        "\n",
        "  optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "    epoch_loss = 0.0\n",
        "\n",
        "    for batch_idx, batch in enumerate(dataloader):\n",
        "      input_ids = batch['input_ids'].to(device)\n",
        "      attention_mask = batch['attention_mask'].to(device)\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      outputs = model(input_ids, attention_mask=attention_mask, labels=input_ids)\n",
        "      loss = outputs.loss\n",
        "      epoch_loss += loss.item()\n",
        "\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      print(f'Batch {batch_idx+1}/{len(dataloader)} Loss: {loss.item()}')\n",
        "\n",
        "    print(f'Epoch {epoch+1}/{num_epochs} Loss: {epoch_loss/len(dataloader)}')\n",
        "\n"
      ],
      "metadata": {
        "id": "BEs02MCzlhzr"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_model(gpt2_model, dataloader, 0.001, 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ve1PZ19klus6",
        "outputId": "f8be868a-e789-4885-c4fa-c7a4f4d6efe8"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 1/250 Loss: 9.96807861328125\n",
            "Batch 2/250 Loss: 0.41449159383773804\n",
            "Batch 3/250 Loss: 0.5355082750320435\n",
            "Batch 4/250 Loss: 0.8849827647209167\n",
            "Batch 5/250 Loss: 0.6395533680915833\n",
            "Batch 6/250 Loss: 0.49662238359451294\n",
            "Batch 7/250 Loss: 0.563899040222168\n",
            "Batch 8/250 Loss: 0.5663633346557617\n",
            "Batch 9/250 Loss: 0.3980436325073242\n",
            "Batch 10/250 Loss: 0.5258944630622864\n",
            "Batch 11/250 Loss: 0.5702910423278809\n",
            "Batch 12/250 Loss: 0.6791684627532959\n",
            "Batch 13/250 Loss: 0.38887205719947815\n",
            "Batch 14/250 Loss: 0.32680636644363403\n",
            "Batch 15/250 Loss: 0.3648949861526489\n",
            "Batch 16/250 Loss: 0.3701474666595459\n",
            "Batch 17/250 Loss: 0.48997610807418823\n",
            "Batch 18/250 Loss: 0.4076242744922638\n",
            "Batch 19/250 Loss: 0.4282248616218567\n",
            "Batch 20/250 Loss: 0.38849762082099915\n",
            "Batch 21/250 Loss: 0.3477745056152344\n",
            "Batch 22/250 Loss: 0.35904189944267273\n",
            "Batch 23/250 Loss: 0.2839856743812561\n",
            "Batch 24/250 Loss: 0.3015250861644745\n",
            "Batch 25/250 Loss: 0.3062423765659332\n",
            "Batch 26/250 Loss: 0.26768290996551514\n",
            "Batch 27/250 Loss: 0.37245601415634155\n",
            "Batch 28/250 Loss: 0.3022201359272003\n",
            "Batch 29/250 Loss: 0.23658263683319092\n",
            "Batch 30/250 Loss: 0.3645854890346527\n",
            "Batch 31/250 Loss: 0.29589760303497314\n",
            "Batch 32/250 Loss: 0.3433704972267151\n",
            "Batch 33/250 Loss: 0.34682440757751465\n",
            "Batch 34/250 Loss: 0.24594400823116302\n",
            "Batch 35/250 Loss: 0.33035144209861755\n",
            "Batch 36/250 Loss: 0.296601802110672\n",
            "Batch 37/250 Loss: 0.3021937906742096\n",
            "Batch 38/250 Loss: 0.3037973642349243\n",
            "Batch 39/250 Loss: 0.35821405053138733\n",
            "Batch 40/250 Loss: 0.3206815719604492\n",
            "Batch 41/250 Loss: 0.21977993845939636\n",
            "Batch 42/250 Loss: 0.30509087443351746\n",
            "Batch 43/250 Loss: 0.2727832496166229\n",
            "Batch 44/250 Loss: 0.2224157303571701\n",
            "Batch 45/250 Loss: 0.242910698056221\n",
            "Batch 46/250 Loss: 0.3551810085773468\n",
            "Batch 47/250 Loss: 0.2935059368610382\n",
            "Batch 48/250 Loss: 0.23789843916893005\n",
            "Batch 49/250 Loss: 0.28089648485183716\n",
            "Batch 50/250 Loss: 0.27481019496917725\n",
            "Batch 51/250 Loss: 0.24869181215763092\n",
            "Batch 52/250 Loss: 0.3240310251712799\n",
            "Batch 53/250 Loss: 0.29996809363365173\n",
            "Batch 54/250 Loss: 0.2528891861438751\n",
            "Batch 55/250 Loss: 0.20525269210338593\n",
            "Batch 56/250 Loss: 0.30022281408309937\n",
            "Batch 57/250 Loss: 0.24565444886684418\n",
            "Batch 58/250 Loss: 0.30365443229675293\n",
            "Batch 59/250 Loss: 0.2593369483947754\n",
            "Batch 60/250 Loss: 0.3083178997039795\n",
            "Batch 61/250 Loss: 0.20567850768566132\n",
            "Batch 62/250 Loss: 0.24447692930698395\n",
            "Batch 63/250 Loss: 0.27002373337745667\n",
            "Batch 64/250 Loss: 0.24532262980937958\n",
            "Batch 65/250 Loss: 0.27560338377952576\n",
            "Batch 66/250 Loss: 0.22422724962234497\n",
            "Batch 67/250 Loss: 0.2333228439092636\n",
            "Batch 68/250 Loss: 0.26331305503845215\n",
            "Batch 69/250 Loss: 0.22106331586837769\n",
            "Batch 70/250 Loss: 0.27984288334846497\n",
            "Batch 71/250 Loss: 0.24852412939071655\n",
            "Batch 72/250 Loss: 0.2849975824356079\n",
            "Batch 73/250 Loss: 0.24364465475082397\n",
            "Batch 74/250 Loss: 0.22924835979938507\n",
            "Batch 75/250 Loss: 0.2763395607471466\n",
            "Batch 76/250 Loss: 0.25753548741340637\n",
            "Batch 77/250 Loss: 0.23056764900684357\n",
            "Batch 78/250 Loss: 0.24834203720092773\n",
            "Batch 79/250 Loss: 0.2645723521709442\n",
            "Batch 80/250 Loss: 0.2429845780134201\n",
            "Batch 81/250 Loss: 0.20237375795841217\n",
            "Batch 82/250 Loss: 0.3114021420478821\n",
            "Batch 83/250 Loss: 0.27969658374786377\n",
            "Batch 84/250 Loss: 0.22432994842529297\n",
            "Batch 85/250 Loss: 0.24211806058883667\n",
            "Batch 86/250 Loss: 0.2555173635482788\n",
            "Batch 87/250 Loss: 0.2370515763759613\n",
            "Batch 88/250 Loss: 0.22937019169330597\n",
            "Batch 89/250 Loss: 0.20478419959545135\n",
            "Batch 90/250 Loss: 0.2246943563222885\n",
            "Batch 91/250 Loss: 0.31162652373313904\n",
            "Batch 92/250 Loss: 0.2535175085067749\n",
            "Batch 93/250 Loss: 0.25089746713638306\n",
            "Batch 94/250 Loss: 0.2124357968568802\n",
            "Batch 95/250 Loss: 0.24216708540916443\n",
            "Batch 96/250 Loss: 0.23089130222797394\n",
            "Batch 97/250 Loss: 0.2327958196401596\n",
            "Batch 98/250 Loss: 0.29042840003967285\n",
            "Batch 99/250 Loss: 0.2121562957763672\n",
            "Batch 100/250 Loss: 0.12763123214244843\n",
            "Batch 101/250 Loss: 0.1998368352651596\n",
            "Batch 102/250 Loss: 0.20403264462947845\n",
            "Batch 103/250 Loss: 0.21543733775615692\n",
            "Batch 104/250 Loss: 0.2176726907491684\n",
            "Batch 105/250 Loss: 0.19977439939975739\n",
            "Batch 106/250 Loss: 0.17990535497665405\n",
            "Batch 107/250 Loss: 0.204835906624794\n",
            "Batch 108/250 Loss: 0.1723988950252533\n",
            "Batch 109/250 Loss: 0.15709111094474792\n",
            "Batch 110/250 Loss: 0.1785036027431488\n",
            "Batch 111/250 Loss: 0.2001384049654007\n",
            "Batch 112/250 Loss: 0.2406041920185089\n",
            "Batch 113/250 Loss: 0.23758572340011597\n",
            "Batch 114/250 Loss: 0.1932412087917328\n",
            "Batch 115/250 Loss: 0.2562021017074585\n",
            "Batch 116/250 Loss: 0.24361135065555573\n",
            "Batch 117/250 Loss: 0.1962413638830185\n",
            "Batch 118/250 Loss: 0.18478859961032867\n",
            "Batch 119/250 Loss: 0.21295934915542603\n",
            "Batch 120/250 Loss: 0.17851994931697845\n",
            "Batch 121/250 Loss: 0.24725620448589325\n",
            "Batch 122/250 Loss: 0.2445731908082962\n",
            "Batch 123/250 Loss: 0.1857803910970688\n",
            "Batch 124/250 Loss: 0.1799711287021637\n",
            "Batch 125/250 Loss: 0.22434812784194946\n",
            "Batch 126/250 Loss: 0.22737829387187958\n",
            "Batch 127/250 Loss: 0.22397837042808533\n",
            "Batch 128/250 Loss: 0.20135921239852905\n",
            "Batch 129/250 Loss: 0.2188434898853302\n",
            "Batch 130/250 Loss: 0.15518221259117126\n",
            "Batch 131/250 Loss: 0.20699378848075867\n",
            "Batch 132/250 Loss: 0.1881779432296753\n",
            "Batch 133/250 Loss: 0.21078553795814514\n",
            "Batch 134/250 Loss: 0.2347746193408966\n",
            "Batch 135/250 Loss: 0.24247832596302032\n",
            "Batch 136/250 Loss: 0.2062140703201294\n",
            "Batch 137/250 Loss: 0.2349967509508133\n",
            "Batch 138/250 Loss: 0.17745287716388702\n",
            "Batch 139/250 Loss: 0.1535070389509201\n",
            "Batch 140/250 Loss: 0.19445490837097168\n",
            "Batch 141/250 Loss: 0.24345536530017853\n",
            "Batch 142/250 Loss: 0.22346735000610352\n",
            "Batch 143/250 Loss: 0.20484676957130432\n",
            "Batch 144/250 Loss: 0.1875031292438507\n",
            "Batch 145/250 Loss: 0.2100561559200287\n",
            "Batch 146/250 Loss: 0.21981225907802582\n",
            "Batch 147/250 Loss: 0.255598247051239\n",
            "Batch 148/250 Loss: 0.1886412799358368\n",
            "Batch 149/250 Loss: 0.19198037683963776\n",
            "Batch 150/250 Loss: 0.17935365438461304\n",
            "Batch 151/250 Loss: 0.17067855596542358\n",
            "Batch 152/250 Loss: 0.229798823595047\n",
            "Batch 153/250 Loss: 0.22768178582191467\n",
            "Batch 154/250 Loss: 0.15754416584968567\n",
            "Batch 155/250 Loss: 0.23253236711025238\n",
            "Batch 156/250 Loss: 0.19066543877124786\n",
            "Batch 157/250 Loss: 0.18003861606121063\n",
            "Batch 158/250 Loss: 0.19064445793628693\n",
            "Batch 159/250 Loss: 0.21127448976039886\n",
            "Batch 160/250 Loss: 0.21670658886432648\n",
            "Batch 161/250 Loss: 0.2472149282693863\n",
            "Batch 162/250 Loss: 0.22674694657325745\n",
            "Batch 163/250 Loss: 0.19980859756469727\n",
            "Batch 164/250 Loss: 0.17653843760490417\n",
            "Batch 165/250 Loss: 0.23566871881484985\n",
            "Batch 166/250 Loss: 0.18059062957763672\n",
            "Batch 167/250 Loss: 0.28365495800971985\n",
            "Batch 168/250 Loss: 0.25185489654541016\n",
            "Batch 169/250 Loss: 0.20320115983486176\n",
            "Batch 170/250 Loss: 0.2488117665052414\n",
            "Batch 171/250 Loss: 0.19482220709323883\n",
            "Batch 172/250 Loss: 0.21483434736728668\n",
            "Batch 173/250 Loss: 0.2201744019985199\n",
            "Batch 174/250 Loss: 0.22043555974960327\n",
            "Batch 175/250 Loss: 0.1883741319179535\n",
            "Batch 176/250 Loss: 0.2415676862001419\n",
            "Batch 177/250 Loss: 0.22778519988059998\n",
            "Batch 178/250 Loss: 0.25097718834877014\n",
            "Batch 179/250 Loss: 0.19983986020088196\n",
            "Batch 180/250 Loss: 0.17971143126487732\n",
            "Batch 181/250 Loss: 0.24204091727733612\n",
            "Batch 182/250 Loss: 0.19194522500038147\n",
            "Batch 183/250 Loss: 0.1962577849626541\n",
            "Batch 184/250 Loss: 0.20661702752113342\n",
            "Batch 185/250 Loss: 0.2191912829875946\n",
            "Batch 186/250 Loss: 0.2105533331632614\n",
            "Batch 187/250 Loss: 0.15247787535190582\n",
            "Batch 188/250 Loss: 0.1561761349439621\n",
            "Batch 189/250 Loss: 0.20034153759479523\n",
            "Batch 190/250 Loss: 0.1967964470386505\n",
            "Batch 191/250 Loss: 0.2184728980064392\n",
            "Batch 192/250 Loss: 0.2237768918275833\n",
            "Batch 193/250 Loss: 0.18358753621578217\n",
            "Batch 194/250 Loss: 0.21668395400047302\n",
            "Batch 195/250 Loss: 0.22346171736717224\n",
            "Batch 196/250 Loss: 0.18125009536743164\n",
            "Batch 197/250 Loss: 0.1864485740661621\n",
            "Batch 198/250 Loss: 0.15712817013263702\n",
            "Batch 199/250 Loss: 0.19872505962848663\n",
            "Batch 200/250 Loss: 0.20681452751159668\n",
            "Batch 201/250 Loss: 0.20927053689956665\n",
            "Batch 202/250 Loss: 0.2316015213727951\n",
            "Batch 203/250 Loss: 0.26852965354919434\n",
            "Batch 204/250 Loss: 0.14935627579689026\n",
            "Batch 205/250 Loss: 0.1957964301109314\n",
            "Batch 206/250 Loss: 0.17440298199653625\n",
            "Batch 207/250 Loss: 0.2208019345998764\n",
            "Batch 208/250 Loss: 0.20362867414951324\n",
            "Batch 209/250 Loss: 0.21089984476566315\n",
            "Batch 210/250 Loss: 0.31159085035324097\n",
            "Batch 211/250 Loss: 0.2145630568265915\n",
            "Batch 212/250 Loss: 0.17497937381267548\n",
            "Batch 213/250 Loss: 0.281440407037735\n",
            "Batch 214/250 Loss: 0.16665439307689667\n",
            "Batch 215/250 Loss: 0.18996933102607727\n",
            "Batch 216/250 Loss: 0.16801296174526215\n",
            "Batch 217/250 Loss: 0.23899304866790771\n",
            "Batch 218/250 Loss: 0.19059067964553833\n",
            "Batch 219/250 Loss: 0.20621466636657715\n",
            "Batch 220/250 Loss: 0.197721928358078\n",
            "Batch 221/250 Loss: 0.243968203663826\n",
            "Batch 222/250 Loss: 0.16039295494556427\n",
            "Batch 223/250 Loss: 0.2658345103263855\n",
            "Batch 224/250 Loss: 0.19096264243125916\n",
            "Batch 225/250 Loss: 0.1831504851579666\n",
            "Batch 226/250 Loss: 0.1904667168855667\n",
            "Batch 227/250 Loss: 0.15676997601985931\n",
            "Batch 228/250 Loss: 0.2372949868440628\n",
            "Batch 229/250 Loss: 0.20530730485916138\n",
            "Batch 230/250 Loss: 0.235231414437294\n",
            "Batch 231/250 Loss: 0.22357118129730225\n",
            "Batch 232/250 Loss: 0.19572168588638306\n",
            "Batch 233/250 Loss: 0.1632816344499588\n",
            "Batch 234/250 Loss: 0.24759550392627716\n",
            "Batch 235/250 Loss: 0.1756884753704071\n",
            "Batch 236/250 Loss: 0.1650819331407547\n",
            "Batch 237/250 Loss: 0.19152557849884033\n",
            "Batch 238/250 Loss: 0.21032840013504028\n",
            "Batch 239/250 Loss: 0.22800633311271667\n",
            "Batch 240/250 Loss: 0.2038184255361557\n",
            "Batch 241/250 Loss: 0.23552057147026062\n",
            "Batch 242/250 Loss: 0.217454195022583\n",
            "Batch 243/250 Loss: 0.16163934767246246\n",
            "Batch 244/250 Loss: 0.23388820886611938\n",
            "Batch 245/250 Loss: 0.21790604293346405\n",
            "Batch 246/250 Loss: 0.1970747858285904\n",
            "Batch 247/250 Loss: 0.20738552510738373\n",
            "Batch 248/250 Loss: 0.18491296470165253\n",
            "Batch 249/250 Loss: 0.1642504632472992\n",
            "Batch 250/250 Loss: 0.19091103971004486\n",
            "Epoch 1/1 Loss: 0.2876328395605087\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_joke(model, tokenizer, input_prompt):\n",
        "\n",
        "  model.eval()\n",
        "\n",
        "  input_ids = tokenizer.encode(input_prompt, return_tensors='pt')\n",
        "  input_ids = input_ids.to(device)\n",
        "\n",
        "  with torch.no_grad():\n",
        "    output = model.generate(\n",
        "        input_ids=input_ids,\n",
        "        attention_mask=torch.ones(input_ids.shape, dtype=torch.long).to(device),\n",
        "        max_length=30000,\n",
        "        # num_return_sequences=3,\n",
        "        num_beams=10,\n",
        "        no_repeat_ngram_size=2,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "\n",
        "        do_sample = True,\n",
        "        temperature = 0.8,\n",
        "        top_k = 25,\n",
        "        top_p = 0.9,\n",
        "    )\n",
        "\n",
        "  generated_joke = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "  return generated_joke"
      ],
      "metadata": {
        "id": "AbOjlTKAnzOm"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "answer = generate_joke(gpt2_model, gpt2_tokenizer, 'Bob')\n",
        "print('Answer:', answer, sep='\\n\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qj5OMeAWoHlw",
        "outputId": "291acba1-6dd1-4a73-9547-31280a9cc191"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer:\n",
            "\n",
            "Bob: What's the difference between a woman and a man? A woman: Well, you can say the same thing. A man: I don't know what you're talking about.\n"
          ]
        }
      ]
    }
  ]
}