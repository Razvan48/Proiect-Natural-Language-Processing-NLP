{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "Wnkk1e20qQN_",
        "outputId": "5cf1d00d-7771-492a-bde9-d69e0ed7b72b"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/sample_data/shortjokes.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-59a3860ffdbc>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mcsv_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/sample_data/shortjokes.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mNUM_JOKES\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/sample_data/shortjokes.csv'"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "csv_file = pd.read_csv('/content/sample_data/shortjokes.csv')\n",
        "\n",
        "NUM_JOKES = 10000\n",
        "jokes = csv_file['Body'].head(NUM_JOKES).to_numpy()\n",
        "train_jokes, val_jokes = train_test_split(jokes, test_size=0.2, random_state=42)\n",
        "print(jokes)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "#from spacy.lang.fr.examples import sentences\n",
        "from typing import List, Tuple\n",
        "import locale\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import Tensor\n",
        "def getpreferredencoding(do_setlocale = True):\n",
        "    return \"UTF-8\"\n",
        "locale.getpreferredencoding = getpreferredencoding\n",
        "# !python -m spacy download en_core_web_md\n",
        "nlp = spacy.load(\"en_core_web_md\")"
      ],
      "metadata": {
        "id": "U_JDOpc9s5ru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
        "      super().__init__()\n",
        "      self.d_model = d_model\n",
        "      self.num_heads = num_heads\n",
        "      # compute head dimension\n",
        "      self.head_dim = d_model // num_heads\n",
        "      # --> d_model = head_dim * num_heads\n",
        "      # so we can still use one linear transformation\n",
        "      # to get q, k, v for all heads in one pass\n",
        "      self.q_proj = nn.Linear(d_model, d_model)\n",
        "      self.k_proj = nn.Linear(d_model, d_model)\n",
        "      self.v_proj = nn.Linear(d_model, d_model)\n",
        "      # output projection to combine heads\n",
        "      self.o_proj = nn.Linear(d_model, d_model)\n",
        "      self.softmax = nn.Softmax(dim=-1)\n",
        "      self.out_dropout = nn.Dropout(dropout)\n",
        "      self.score_dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "      \"\"\"\n",
        "      :param x: input of size batch_size x seq_len x d_model\n",
        "      :param mask: attn mask of size seq_len x seq_len\n",
        "      :returns: output z of size batch_size x seq_len x d_model\n",
        "              attn_scores of size batch_size x num heads x seq_len x seq_len\n",
        "      \"\"\"\n",
        "      #TODO: compute queries, keys, and values\n",
        "      # q = ...\n",
        "      # k = ...\n",
        "      # v = ...\n",
        "      q = self.q_proj(x)\n",
        "      k = self.k_proj(x)\n",
        "      v = self.v_proj(x)\n",
        "\n",
        "      # split qkv into individual heads\n",
        "      bs, seq_len, _ = x.size()\n",
        "      # bs x seq_len x d_model --> bs x num_heads x seq_len x head_dim\n",
        "      q = q.view(bs, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "      k = k.view(bs, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "      v = v.view(bs, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "\n",
        "      # TODO: just good old self-attention\n",
        "      attn_scores = q @ k.transpose(-2, -1)\n",
        "      attn_scores = attn_scores / math.sqrt(self.head_dim)\n",
        "      if mask is not None:\n",
        "          attn_scores += mask\n",
        "      attn_scores = self.softmax(attn_scores)\n",
        "      attn_scores = self.score_dropout(attn_scores)\n",
        "\n",
        "      output = attn_scores @ v\n",
        "\n",
        "      # combine heads\n",
        "      # bs x num_heads x seq_len x head_dim  --> bs x seq_len x d_model\n",
        "      output = output.transpose(1, 2).contiguous()\n",
        "      output = output.view(bs, seq_len, self.head_dim*self.num_heads)\n",
        "\n",
        "      #TODO: apply output projection(wO)\n",
        "      output = self.o_proj(output)\n",
        "      output = self.out_dropout(output)\n",
        "\n",
        "      return output, attn_scores"
      ],
      "metadata": {
        "id": "wBZc3YkW0Be3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
        "        div_term = torch.exp(\n",
        "            torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model)\n",
        "        )\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer(\"pe\", pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # embeddings + possitional embeddings\n",
        "        x = x + self.pe[:, :x.size(1)]\n",
        "        return self.dropout(x)"
      ],
      "metadata": {
        "id": "kvFdWTwf2j-1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "  def __init__(self, d_model, d_ff=None, dropout=0.1):\n",
        "    super().__init__()\n",
        "    self.d_ff = d_ff if d_ff else 4 * d_model\n",
        "    self.w_1 = nn.Linear(d_model, self.d_ff)\n",
        "    self.w_2 = nn.Linear(self.d_ff, d_model)\n",
        "    self.activation = nn.ReLU()\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, x):\n",
        "    \"\"\"\n",
        "    :param x: input of size batch_size x seq_len x d_model\n",
        "    :returns: FFN representtions, same shape as input\n",
        "    \"\"\"\n",
        "    #TODO: pass x through the FFN (see equation above)\n",
        "    x = self.w_2(self.activation(self.w_1(x)))\n",
        "    x = self.dropout(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "In_JgCGy25ZQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerLayer(nn.Module):\n",
        "  def __init__(self, d_model, num_heads, d_ff=None, dropout=0.1):\n",
        "    super().__init__()\n",
        "    self.d_model = d_model\n",
        "    self.mha = MultiHeadAttention(\n",
        "        d_model=d_model,\n",
        "        num_heads=num_heads,\n",
        "        dropout=dropout\n",
        "    )\n",
        "    self.ffn = FeedForward(\n",
        "        d_model=d_model,\n",
        "        d_ff=d_ff,\n",
        "        dropout=dropout\n",
        "    )\n",
        "    #layer norms\n",
        "    self.norm_attn = nn.LayerNorm(d_model)\n",
        "    self.norm_ffn = nn.LayerNorm(d_model)\n",
        "\n",
        "  def forward(self, x, mask=None):\n",
        "    attn_output, attn_scores = self.mha(x, mask)\n",
        "    #TODO: write the residual\n",
        "    x = self.norm_attn(x + attn_output)\n",
        "    x = self.norm_ffn(x + self.ffn(x))\n",
        "    return x, attn_scores"
      ],
      "metadata": {
        "id": "LZSCtyJ37yHy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LENGTH = 250\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "from transformers import BertTokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "def tokenize(text):\n",
        "    return tokenizer.tokenize(text)\n",
        "\n",
        "class Vocabulary:\n",
        "    \"\"\"\n",
        "    Helper class that maps words to unique indices and the other way around\n",
        "    \"\"\"\n",
        "    def __init__(self, samples: List[str]):\n",
        "        vocabulary = set()\n",
        "        for sample in samples:\n",
        "          tokens = tokenize(sample)\n",
        "          vocabulary.update(tokens)\n",
        "\n",
        "        self.token_to_idx = {token:idx for (idx, token)\n",
        "                            in enumerate(vocabulary)}\n",
        "        self.idx_to_token = {idx:token for (idx, token)\n",
        "                            in enumerate(vocabulary)}\n",
        "\n",
        "    def size(self):\n",
        "        return len(self.token_to_idx)\n",
        "\n",
        "    def __str__(self):\n",
        "        return str(self.token_to_idx)\n",
        "\n",
        "vocab = Vocabulary(jokes)\n",
        "print(\"Vocabulary size: \", vocab.size())\n",
        "print(\"Vocabulary: \\n\", vocab)"
      ],
      "metadata": {
        "id": "osPedtpc-F2f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def text_to_tensor(text: str, vocab: Vocabulary) -> torch.LongTensor:\n",
        "    \"\"\"\n",
        "    Convert a string to a Tensor with corresponding character indices\n",
        "    e.g. \"We have\" -> [48, 13]\n",
        "    \"\"\"\n",
        "    #TODO\n",
        "    token_ids = [vocab.token_to_idx[token] for token in tokenize(text)]\n",
        "    return torch.tensor(token_ids)\n",
        "\n",
        "def tensor_to_text(x: torch.LongTensor, vocab: Vocabulary) -> str:\n",
        "    \"\"\"\n",
        "    Convert a Tensor of token indices to its string representation\n",
        "    e.g. [48, 13] -> \"We have\"\n",
        "    \"\"\"\n",
        "    #TODO\n",
        "    return \"\".join(vocab.idx_to_token[idx.item()] for idx in x)"
      ],
      "metadata": {
        "id": "5kSQ3c3HIt-D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_sequences(jokes, vocab, max_length=250):\n",
        "    sequences = []\n",
        "    for joke in jokes:\n",
        "        tokens = text_to_tensor(joke, vocab)\n",
        "        sequences.append(tokens)\n",
        "    return sequences\n",
        "\n",
        "train_sequences = get_sequences(train_jokes, vocab, max_length=MAX_LENGTH)\n",
        "val_sequences = get_sequences(val_jokes, vocab, max_length=MAX_LENGTH)\n",
        "\n",
        "print(\"Train sequences:\", len(train_sequences))\n",
        "print(\"Val sequences:\", len(val_sequences))\n"
      ],
      "metadata": {
        "id": "oGBGbrjhIzFE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "class StoriesDataset(Dataset):\n",
        "    def __init__(self, sequences):\n",
        "        self.data = sequences\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        sequence = self.data[idx]\n",
        "        x = torch.LongTensor(sequence)\n",
        "        input = x[:-1]\n",
        "        label = x[1:] # shift to the right\n",
        "        return input, label\n",
        "\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "def collate_fn(batch):\n",
        "    inputs, labels = zip(*batch)  # list of (input, label)\n",
        "    inputs_padded = pad_sequence(inputs, batch_first=True, padding_value=0)\n",
        "    labels_padded = pad_sequence(labels, batch_first=True, padding_value=0)\n",
        "    return inputs_padded, labels_padded\n"
      ],
      "metadata": {
        "id": "utL00yXJPh8g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n"
      ],
      "metadata": {
        "id": "9xhpQb31RLFB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x, y = next(iter(train_loader))\n",
        "print('Batch input shape:', x.shape)\n",
        "print('Batch label shape:', y.shape)\n"
      ],
      "metadata": {
        "id": "ERUc-4AlSE1f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_hyperparameters_dict = {\n",
        "    \"batch_size\": BATCH_SIZE, # =32\n",
        "    \"num_epochs\": 3,\n",
        "    \"max_len\": MAX_LENGTH, # =250\n",
        "    \"embedding_size\": 256,\n",
        "    \"learning_algo\": \"adam\",\n",
        "    \"learning_rate\": 1e-4,\n",
        "    \"max_grad_norm\": 5.0,\n",
        "    \"num_layers\": 6\n",
        "}\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "metadata": {
        "id": "HyDR1RsvSOUB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerLM(nn.Module):\n",
        "    def __init__(self, vocab_size: int, d_model: int,\n",
        "                 dropout_p: float, seq_len: int, num_layers: int):\n",
        "        super().__init__()\n",
        "        self.name = 'transformer'\n",
        "        self.vocab_size = vocab_size\n",
        "        self.d_model = d_model\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "        self.seq_len = seq_len\n",
        "        self.num_layers = num_layers\n",
        "        self.norm = nn.LayerNorm(d_model)\n",
        "\n",
        "        # TODO: instantiate Modules with the correct arguments\n",
        "        # self.embedding = nn.Embedding(...)\n",
        "        self.embedding = nn.Embedding(\n",
        "            num_embeddings=vocab_size,\n",
        "            embedding_dim=d_model\n",
        "        )\n",
        "\n",
        "        # self.positional_encoding = PositionalEncoding(\n",
        "        #     ...\n",
        "        # )\n",
        "\n",
        "        self.positional_encoding = PositionalEncoding(\n",
        "            d_model=d_model,\n",
        "            dropout=dropout_p\n",
        "        )\n",
        "\n",
        "        # stack num_layers transformer layers\n",
        "        self.transformer = nn.ModuleList([\n",
        "            TransformerLayer(d_model=d_model,\n",
        "            num_heads=8,\n",
        "            dropout=dropout_p)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        # instantiate the output layer\n",
        "        # self.output_layer = nn.Linear(\n",
        "        #     ...\n",
        "        # )\n",
        "        self.output_layer = nn.Linear(\n",
        "            in_features=d_model,\n",
        "            out_features=vocab_size\n",
        "        )\n",
        "\n",
        "        # https://paperswithcode.com/method/weight-tying\n",
        "        self.embedding.weight = self.output_layer.weight\n",
        "\n",
        "    def forward(self, x: torch.LongTensor) -> torch.FloatTensor:\n",
        "        \"\"\"\n",
        "        :param x: input of size batch_size x max_len\n",
        "        :return: logits of size batch_size x seq_len x vocab_size\n",
        "        \"\"\"\n",
        "        seq_len = x.size(1)\n",
        "        # compute embeddings, then multiply them by sqrt(d_model)\n",
        "        #x = ...\n",
        "        x = self.embedding(x) * math.sqrt(self.d_model)\n",
        "        # add positional_encodings\n",
        "        x = self.positional_encoding(x)\n",
        "\n",
        "        # generate a causal mask for the sequence\n",
        "        # masked positions are filled with -inf\n",
        "        mask = nn.Transformer.generate_square_subsequent_mask(seq_len).to(device)\n",
        "\n",
        "        for layer in self.transformer:\n",
        "          x, attn_scores = layer(x, mask=mask)\n",
        "\n",
        "        # apply layernorm to final transformer output\n",
        "        x = self.norm(x)\n",
        "\n",
        "        # output layer\n",
        "        # logits = ...\n",
        "        logits = self.output_layer(x)\n",
        "\n",
        "        return logits"
      ],
      "metadata": {
        "id": "ZuSfNkhzTOqL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.randint(0, vocab.size(), (1, MAX_LENGTH)).to(device)\n",
        "model = TransformerLM(\n",
        "    d_model = _hyperparameters_dict[\"embedding_size\"],\n",
        "    seq_len = _hyperparameters_dict[\"max_len\"],\n",
        "    num_layers = 6,\n",
        "    dropout_p = 0.1,\n",
        "    vocab_size = vocab.size()\n",
        ").to(device)\n",
        "output = model(x)\n",
        "print(output.shape)"
      ],
      "metadata": {
        "id": "QGjqp-4vTldO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print('Trainable params', trainable_params)"
      ],
      "metadata": {
        "id": "NzJWZ4QrT-M1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim import SGD, Adam\n",
        "\n",
        "class Trainer:\n",
        "    def __init__(self, model: nn.Module,\n",
        "                 train_loader: DataLoader,\n",
        "                 val_loader: DataLoader,\n",
        "                 vocab: Vocabulary,\n",
        "                 hyperparams: dict,\n",
        "                 num_train_examples: int,\n",
        "                 num_val_examples: int):\n",
        "        self.model = model\n",
        "        self.train_loader = train_loader\n",
        "        self.val_loader = val_loader\n",
        "        self.vocab = vocab\n",
        "        self.optimizer = Adam(params = self.model.parameters(),\n",
        "                              lr = hyperparams['learning_rate'])\n",
        "        self.num_epochs = hyperparams['num_epochs']\n",
        "        self.max_len = hyperparams['max_len']\n",
        "        self.batch_size = hyperparams['batch_size']\n",
        "        self.max_grad_norm = hyperparams['max_grad_norm']\n",
        "        self.num_train_examples = num_train_examples\n",
        "        self.num_val_examples = num_val_examples\n",
        "        # loss for output layer\n",
        "        self.loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "    def train_epoch(self, epoch_num: int) -> float:\n",
        "        \"\"\"\n",
        "        Compute the loss on the training set\n",
        "        :param epoch_num: number of current epoch\n",
        "        \"\"\"\n",
        "        self.model.train()\n",
        "        epoch_loss = 0.0\n",
        "        for batch_num, (x, y) in enumerate(train_loader):\n",
        "\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            # reset gradients\n",
        "            self.optimizer.zero_grad()\n",
        "\n",
        "            # unnormalized probabilities\n",
        "            # batch x seq_len x vocab_size\n",
        "            logits = self.model(x)\n",
        "\n",
        "            # TODO: compute loss\n",
        "            # remember, we are making next token predictions over the vocab\n",
        "            # for each token in the sequence, for each sequence in the batch\n",
        "            # !! so we need to flatten the first two dimensions before cross entropy\n",
        "            batch_loss = self.loss_fn(\n",
        "                # batch x seq_len x vocab_size --> (batch*seq_len) x vocab_size\n",
        "                logits.view(-1, logits.size(-1)),\n",
        "                # batch x seq_len --> batch*seq_len\n",
        "                y.view(-1)\n",
        "            )\n",
        "\n",
        "            epoch_loss += batch_loss.item()\n",
        "\n",
        "            # backpropagation (gradient of loss wrt parameters)\n",
        "            batch_loss.backward()\n",
        "\n",
        "            # clip gradients if they get too large\n",
        "            torch.nn.utils.clip_grad_norm_(list(self.model.parameters()),\n",
        "                                           self.max_grad_norm)\n",
        "\n",
        "            # update parameters\n",
        "            self.optimizer.step()\n",
        "\n",
        "            if batch_num % 100 == 0:\n",
        "                print(\"epoch %d, %d/%d examples, batch loss = %f\"\n",
        "                      % (epoch_num, (batch_num + 1) * self.batch_size,\n",
        "                         self.num_train_examples, batch_loss.item()))\n",
        "        epoch_loss /= (batch_num + 1)\n",
        "\n",
        "        return epoch_loss\n",
        "\n",
        "    def eval_epoch(self, epoch_num: int) -> float:\n",
        "        \"\"\"\n",
        "        Compute the loss on the validation set\n",
        "        :param epoch_num: number of current epoch\n",
        "        \"\"\"\n",
        "        epoch_loss = 0.0\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            for batch_num, (x, y) in enumerate(val_loader):\n",
        "\n",
        "                x, y = x.to(device), y.to(device)\n",
        "                #batch x timesteps x vocab_size\n",
        "                logits = self.model(x)\n",
        "\n",
        "                # TODO: compute loss\n",
        "                # remember, we are making next token predictions over the vocab\n",
        "                # for each token in the sequence, for each sequence in the batch\n",
        "                # !! so we need to flatten the first two dimensions before cross entropy\n",
        "                batch_loss = self.loss_fn(\n",
        "                    # batch x seq_len x vocab_size --> (batch*seq_len) x vocab_size\n",
        "                    logits.view(-1, logits.size(-1)),\n",
        "                    # batch x seq_len --> batch*seq_len\n",
        "                    y.view(-1)\n",
        "                )\n",
        "                epoch_loss += batch_loss.item()\n",
        "\n",
        "            epoch_loss /= (batch_num + 1)\n",
        "\n",
        "        return epoch_loss\n",
        "\n",
        "    def train(self) -> dict:\n",
        "        train_losses, val_losses = [], []\n",
        "        for epoch in range(self.num_epochs):\n",
        "            epoch_train_loss = self.train_epoch(epoch)\n",
        "            epoch_val_loss = self.eval_epoch(epoch)\n",
        "            train_losses.append(epoch_train_loss)\n",
        "            val_losses.append(epoch_val_loss)\n",
        "        return {\"train_losses\": train_losses,\n",
        "                \"val_losses\": val_losses}\n",
        "\n",
        "def plot_losses(metrics: dict):\n",
        "    \"\"\"\n",
        "    Plots training/validation losses.\n",
        "    :param metrics: dictionar\n",
        "    \"\"\"\n",
        "    plt.figure()\n",
        "    plt.plot(metrics['train_losses'], c='b', label='Train')\n",
        "    plt.plot(metrics['val_losses'], c='g', label='Valid')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.xlabel('Iteration')\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "CWYI7Y5MUFI3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train the transformer for three epochs, should take about 10-15 mins\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    train_loader=train_loader,\n",
        "    val_loader=val_loader,\n",
        "    vocab=vocab,\n",
        "    hyperparams= _hyperparameters_dict,\n",
        "    num_train_examples=len(train_sequences),\n",
        "    num_val_examples=len(val_sequences)\n",
        ")\n",
        "metrics = trainer.train()\n",
        "plot_losses(metrics)"
      ],
      "metadata": {
        "id": "iQkWK6CPURUB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}