{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "OT_BVU6KAex4"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "import torch\n",
        "import spacy\n",
        "import torch.nn as nn\n",
        "\n",
        "import locale\n",
        "\n",
        "def get_preferred_encoding(do_set_locale=True):\n",
        "  return 'UTF-8'\n",
        "locale.getpreferredencoding = get_preferred_encoding"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_md"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rBb7flmbIdrS",
        "outputId": "42c45dec-bb61-43da-8234-d332652540f2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-md==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.8.0/en_core_web_md-3.8.0-py3-none-any.whl (33.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.5/33.5 MB\u001b[0m \u001b[31m65.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_md')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wRDCPZSDIDXr",
        "outputId": "d3b2efab-e459-437d-db01-63832dba2fe1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "csv_file = pd.read_csv('/content/drive/MyDrive/Proiect NLP/Preprocessed-Datasets/shortjokes/shortjokes.csv')\n",
        "\n",
        "NUM_JOKES = 50\n",
        "data = csv_file['Body'].head(NUM_JOKES).to_numpy()\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "pNkqY2ZLIHOl"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = spacy.load('en_core_web_md')"
      ],
      "metadata": {
        "id": "64MJTQI_ITHr"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_and_embed(jokes, llm):\n",
        "\n",
        "  tokens = []\n",
        "  embeddings = []\n",
        "  for joke_idx, joke in enumerate(jokes):\n",
        "\n",
        "    output = llm(joke)\n",
        "    tokens.append([out.text for out in output])\n",
        "    embeddings.append(torch.stack([torch.tensor(out.vector) for out in output]))\n",
        "\n",
        "    if (joke_idx + 1) % 250 == 0:\n",
        "      print(f'Joke {joke_idx + 1}/{len(jokes)} tokenized and embedded')\n",
        "\n",
        "  return tokens, embeddings"
      ],
      "metadata": {
        "id": "AAs5QjweI80f"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens, embeddings = tokenize_and_embed(data, llm)"
      ],
      "metadata": {
        "id": "fWpGDE6ZJuvP"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RNNModel(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, output_size):\n",
        "    super(RNNModel, self).__init__()\n",
        "\n",
        "    self.input_size = input_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.output_size = output_size\n",
        "\n",
        "    self.rnn = nn.GRU(\n",
        "        input_size=self.input_size,\n",
        "        hidden_size=self.hidden_size,\n",
        "        num_layers=3,\n",
        "        batch_first=True\n",
        "    )\n",
        "\n",
        "    self.output_layer = nn.Linear(\n",
        "        in_features=self.hidden_size,\n",
        "        out_features=self.output_size\n",
        "    )\n",
        "\n",
        "  def forward(self, input, hidden_state):\n",
        "    rnn_output, hidden_state = self.rnn(input, hidden_state)\n",
        "    output = self.output_layer(rnn_output)\n",
        "\n",
        "    return output, hidden_state"
      ],
      "metadata": {
        "id": "CAVqk6oDj59O"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EMBEDDING_SIZE = llm.vocab.vectors_length\n",
        "\n",
        "rnn_model = RNNModel(\n",
        "    input_size=EMBEDDING_SIZE,\n",
        "    hidden_size=512,\n",
        "    output_size=EMBEDDING_SIZE,\n",
        ")"
      ],
      "metadata": {
        "id": "7Zf_WdO7ktke"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, tokens, embeddings, learning_rate, num_epochs):\n",
        "\n",
        "  model.train()\n",
        "\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "    epoch_loss = 0.0\n",
        "\n",
        "    for joke_idx, joke in enumerate(tokens):\n",
        "\n",
        "      current_hidden_state = None\n",
        "\n",
        "      for token_idx, token in enumerate(joke):\n",
        "        if token_idx == len(joke) - 1:\n",
        "          break\n",
        "\n",
        "        current_token_embedding = embeddings[joke_idx][token_idx]\n",
        "        current_token_embedding = current_token_embedding.view(1, 1, -1)\n",
        "\n",
        "        next_token_embedding = embeddings[joke_idx][token_idx + 1]\n",
        "        next_token_embedding = next_token_embedding.view(1, 1, -1)\n",
        "\n",
        "        if current_hidden_state is not None:\n",
        "          current_hidden_state = current_hidden_state.detach()\n",
        "\n",
        "        output, current_hidden_state = model(current_token_embedding, current_hidden_state)\n",
        "\n",
        "        loss = nn.functional.mse_loss(output, next_token_embedding)\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # print(f'Trained on joke {joke_idx + 1}/{len(tokens)}, token {token_idx + 1}/{len(joke)}')\n",
        "\n",
        "      print(f'Trained on joke {joke_idx + 1}/{len(tokens)}')\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs} Loss: {epoch_loss / len(tokens)}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "OHR2WMg4lSNk"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_model(rnn_model, tokens, embeddings, 0.001, 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MIHQ96qmoxWq",
        "outputId": "8b461408-bfc8-49a8-c5aa-8a75a28830c4"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trained on joke 1/50\n",
            "Trained on joke 2/50\n",
            "Trained on joke 3/50\n",
            "Trained on joke 4/50\n",
            "Trained on joke 5/50\n",
            "Trained on joke 6/50\n",
            "Trained on joke 7/50\n",
            "Trained on joke 8/50\n",
            "Trained on joke 9/50\n",
            "Trained on joke 10/50\n",
            "Trained on joke 11/50\n",
            "Trained on joke 12/50\n",
            "Trained on joke 13/50\n",
            "Trained on joke 14/50\n",
            "Trained on joke 15/50\n",
            "Trained on joke 16/50\n",
            "Trained on joke 17/50\n",
            "Trained on joke 18/50\n",
            "Trained on joke 19/50\n",
            "Trained on joke 20/50\n",
            "Trained on joke 21/50\n",
            "Trained on joke 22/50\n",
            "Trained on joke 23/50\n",
            "Trained on joke 24/50\n",
            "Trained on joke 25/50\n",
            "Trained on joke 26/50\n",
            "Trained on joke 27/50\n",
            "Trained on joke 28/50\n",
            "Trained on joke 29/50\n",
            "Trained on joke 30/50\n",
            "Trained on joke 31/50\n",
            "Trained on joke 32/50\n",
            "Trained on joke 33/50\n",
            "Trained on joke 34/50\n",
            "Trained on joke 35/50\n",
            "Trained on joke 36/50\n",
            "Trained on joke 37/50\n",
            "Trained on joke 38/50\n",
            "Trained on joke 39/50\n",
            "Trained on joke 40/50\n",
            "Trained on joke 41/50\n",
            "Trained on joke 42/50\n",
            "Trained on joke 43/50\n",
            "Trained on joke 44/50\n",
            "Trained on joke 45/50\n",
            "Trained on joke 46/50\n",
            "Trained on joke 47/50\n",
            "Trained on joke 48/50\n",
            "Trained on joke 49/50\n",
            "Trained on joke 50/50\n",
            "Epoch 1/1 Loss: 1.891767866499722\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def find_most_similar_token(current_token_embedding, llm):\n",
        "  EPSILON = 0.001\n",
        "  max_similarity = -1 - EPSILON\n",
        "  most_similar_token = None\n",
        "\n",
        "  current_token_embedding = current_token_embedding.squeeze().reshape(EMBEDDING_SIZE)\n",
        "\n",
        "  for token in llm.vocab:\n",
        "    if not token.has_vector:\n",
        "      continue\n",
        "\n",
        "    token_embedding = torch.tensor(token.vector)\n",
        "\n",
        "    similarity = nn.functional.cosine_similarity(current_token_embedding, token_embedding, dim=0)\n",
        "\n",
        "    if similarity > max_similarity:\n",
        "      max_similarity = similarity\n",
        "      most_similar_token = token.text\n",
        "\n",
        "  return most_similar_token\n",
        "\n",
        "\n",
        "def generate_joke(model, llm, joke_num_tokens):\n",
        "\n",
        "  joke = []\n",
        "\n",
        "  model.eval()\n",
        "\n",
        "  current_hidden_state = None\n",
        "\n",
        "  current_token_embedding = torch.zeros(1, 1, EMBEDDING_SIZE)\n",
        "\n",
        "  for token in range(joke_num_tokens):\n",
        "\n",
        "    current_token_embedding, current_hidden_state = model(current_token_embedding, current_hidden_state)\n",
        "\n",
        "    joke.append(find_most_similar_token(current_token_embedding, llm))\n",
        "\n",
        "  return ' '.join(joke)"
      ],
      "metadata": {
        "id": "LDkTbUWupc8S"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "joke = generate_joke(rnn_model, llm, 100)\n",
        "print('Answer:', joke, sep='\\n\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eqFIDZlxq66j",
        "outputId": "4d7a53c2-b73f-4f5f-8f56-a976df030d05"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer:\n",
            "\n",
            "if if if if if if if if if if if if if if if if if if if if if if if if if if if if if if if if if if if if if if if if if if if if if if if if if if if if if if if if if if if if if if if if if if if if if if if if if if if if if if if if if if if if if if if if if if if if if if if if if if if if\n"
          ]
        }
      ]
    }
  ]
}